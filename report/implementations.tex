\title{Implementations}

\vspace{0.6 cm}

\noindent
This chapter presents the various implementations of the data validaion rules described in chapter \ref{implementations}.

\section{VTL}

\textsc{Laura Vignola}
\vspace{0.6 cm}

VTL is a standard language for defining validation and transformation rules. It was launched at the end of 2012 by the SDMX Secretariat and on February 2015, the version 1.0 of VTL has been released as a package containing: a document with the main characteristics of VTL (VTL part 1); a document with the full library of operators (VTL part 2) and the VTL technical notation (Extended Backus-Naur Form notation). Figure~\ref{vtldatamod} shows the data model of VTL:

\begin{figure}[!ht]
\begin{center}
\includegraphics[scale=0.5]{fig/vtldatamod.png} 
\end{center}
\caption{ER diagramm of the VTL data model}
\label{vtldatamod}
\end{figure}

The general characteristics of VTL are to:
\begin{itemize}
\item
Be user oriented: to be used by also non IT people and to be oriented to statistics, to be intuitive and friendly.

\item
Have an integrated approach: to be independent of statistical domain,  statistical process phase and typologies of data (micro and macro, registers data, survey data etc.). To be independent from the standard: the VTL Information model can be mapped with those of different standards like SDMX, GSIM and DDI.

\item
Have an active role for processing, driving in an active way the execution of the calculation.

\item
To be independent from the IT implementation, allowing implementations with different programming languages.

\end{itemize}

VTL defines a set of operators that can be applied to dataset or constant or both. A VTL dataset can be considered as a logical table based on a Data Structure having independent variables (identifiers) and dependent variables (measures, attributes). This is the reason why, when we use VTL with datasets (like in the PoC examples) , it is necessary to define the Data Structures related to the dataset, specifying  the components representing your data and which of them are identifiers or measures or attributes.

A VTL expression transforms one or more datasets (or constants) in an output dataset (or constant) that can also be the input of a subsequent transformation.
The translations provided in the PoC are based on the following VTL constraints:

\begin{itemize}

\item
A dataset must always have at least an identifier so that is not possible aggregate data having as result  a unique constant value.

\item
Some operators work on the match of the name of measures  and type. In particular when they act on the whole datasets. For example the sum of two datasets can be performed only if they have the same measures in name and type.

\item
VTL manages the output in different way: when the operators act on the same measure, the measure itself is overwritten with the result, in case of boolean operators it creates a specific component (“CONDITION”) and it allows the creation of new components through the “calc” operator.

\end{itemize}

In the next future a new version of VTL will be released taking into account the results and comments of the PoC experience.

\section{Validate}

The \code{validate} package \citep{loo:2015} is build on top of the popular and
R language for statistical computing \citep{rcore:2015}. It is
implemented as an `R package' -- a strictly standardized way of distributing R
software, and publicly available through the Comprehensive R Archive Network
(CRAN). 

\subsubsection{The R language and environment}
The R language is an open source project, supported by the R foundation seated
in Vienna, Austria\footnote{\code{https://www.r-project.org/foundation/}}.
Over the last decade R received a surge in popularity both from research and
data science and business analytics communities. As a result R currently
integrates with every popular software system for data storage and processing,
including Microsoft SQL server, Oracle databases, Tibco Spotfire, Spark, and
Hadoop to name but a few.  Interaction with other programming environments such
as \code{C} and \code{C++}, \code{Java}, \code{.Net}, and \code{javascript} are
readily and freely available as well. The recently established R
consortium\footnote{\code{https://www.r-consortium.org/}} is an industry
initiative aimed to streamline and fund further developments in R.


The validate package is intended to be a small and powerful package that can be
used standalone by an analyst using R, or integrated easily into bigger
projects using one of the facilities mentioned above. As such, it is build with
strong adherence to the famous Unix
philosophy\footnote{\code{https://en.wikipedia.org/wiki/Unix\_philosophy}}: do
one thing, and do it really well.


\subsubsection{Approach for validate}
The philosophy behind the validate package is to reuse existing solutions as
much as possible. Most importantly, this means that rather than defining a
language from scratch, the validate package utilizes a subset of existing R
syntax for defining data validation rules. As an example, consider the following rule from the ESSnet survey \citep{walsdorfer:2015}.
\begin{quote}
Check whether the relative occurrence of the category high in a column
containing values low, high, medium does not exceed 10%.
\end{quote}
We assume a simple test dataset, with at least a column called \code{level}.
\begin{center}
\begin{tabular}{|c|}
\hline
\textbf{level}\\
\hline
medium\\
\hline
low\\
\hline
medium\\
\hline
medium\\
\hline
high\\
\hline
$\vdots$\\
\hline
\end{tabular}
\end{center}
In validate, the above rule translates to the following syntax.
\begin{verbatim}
  counts :=  table(level)
  counts["high"] < 0.1 * sum(counts)
\end{verbatim}
Here, \code{table} is a cross tabulation function that tabulates occurrences
of values in the \code{level} column. Since this function is a standard R
feature, tabulation or counting is not part of validate's syntax. Indeed the
validate package only makes sure that the statements issued by a user are in
fact validating statements as defined in the Handbook on validation developed
for this project \citep{zio:2015}. In particular, this means that every
(statistical) transformation available to R can be part of a validating
statements.  With thousands of functions in base R, combined with the more than
7600 user-contributed packages available on CRAN, this results in a truly rich
set of validation functionality with a minimal cost of development and
maintenance to the authors.

Finally, we note that the approach of reusing R syntax for a specific purpose
is not new. Indeed several authors have created popular packages using the same
technology. Examples include an implementation of the famous grammar of
graphics \citep{wilkinson:2006} by \citet{wickham:2014}, the  `formula
interface' for statistical model specification and a grammar for data
manipulation by \cite{wickham:2014}. In fact, reusing existing languages, in
particular functional languages like R, is a well-established technique in
computer science.  Some key references for developing such embedded domain
specific languages (DSL) are \cite{fowler:2011} and \cite{gibbons:2015}.


\subsubsection{Features and usage}
Given that statistical functionality comes for free with R, the validate
package focuses only on features directly related to execution and maintenance
of validation rules and  analyses and presentation of validation results.
A few of the features are highlighted below.

\begin{description}
\item[Rule definition.] Rules can be defined on the R commandline for interactive
use or in text files that can be stored and manipulated as source code. Rule sets
may also be stored in a data base, since R by default supports connectivity to 
(nearly) any database that is (commercially) available.

\item[Rule metadata.] If so desired, rules can be provided with metadata such as
a time stamp, short and long descriptions, and the origin of the rule.  By
default, validate uses the widely supported YAML format for metadata
\citep{ben:2009}, mainly beacause of its human-readability. Since YAML can be
translated to formats like XML or JSON at the push of a button, communication
of rules between different softwares or machines is very easy.

\item[Rule organisation.] Dependencies between text files defining rules are
supported to allow for easy reuse of rules across different datasets.  A type
of \code{include} statement allows for hierarchical dependencies between rule
sets.

\item[Rule execution.] Validation rules can be confronted with any data that can
be read into R, which encompasses every common data format currently in use.
Of particular interest is \code{SDMX}, which is supported for R through a
package of \cite{bondel:2015}.

\item[Rule analyses.] Automatic detection of rule types, variable occurrence and
the rule dependency graph are currently built in. Add-on packages are currently
prototyped that will allow for detection of redundancies or contradictory rule
sets.

\item[Extensibility.] This is borrowed from R's extensibility. Users may define
functions which can be used in validating statements. The package needs no special
provisions for that.

\item[Integration.] Integration and communication with other is borrowed from
the strong features that R already has in this area. Because of its openness,
many interfaces with existing software and platforms, including commercial
ones, have been developed.

\item[Analyses of results.] The package has basic functionality to aggregate and
visualize results of validation  procedures. For example, one can sort records
according to the number of rule violations (record prioritization) or rules
according to which have been violated most often. Such summaries, and visualisations
thereof are important tools for monitoring data quality and the efficacy of 
data cleaning process steps \citep{pannekoek:2014}.
\end{description}

The \code{validate} package is the successor of an earlier package of the same
authors named \code{editrules}. The latter package has been used in production
for several years at Statistics Netherlands and is being evaluated by several
other national statistical institutes. At the moment, validate is being
introduced as editrules' replacement. The validate package is backwards
compatible with editrules in that it can read the same input files. However,
validate extends the earlier package by allowing for a much wider range of
validation rules (editrules was restricted to certain types of in-record
validation rules that are suitable for an error-localization algorithm).




\section{eSTATISTIK}

\subsubsection{The wider context}
The term \textit{eSTATISTIK} stands for a wider modernisation approach in German official statistics that has been ongoing for more than a decade and that aims at organising the transition from paper based to electronic production workflows and at the same time maximising its benefits in terms of efficiency, cost-effectiveness, flexibility, speed and quality. It involves all 15 German statistical institutes and therefore faces many issues that appear in a similar fashion at the European and international level. From a strategic perspective, standardisation in general and the movement from stove-pipe to cross-domain processes are important elements. This has led to the creation of generic applications and XML document types covering most steps of the production process. Specifically, the data collection process can be fully implemented using only generic metadata-driven applications, including data validation components for front-end and back-end systems.

Another important strategic element is the consolidation and optimisation of the collaboration of the official statistical institutes and their partners. Its implementation requires adequate infrastructures for sharing and exchanging data and metadata throughout development and production life cycles, application hosting and sharing and the avoidance of duplicate work.

In the development and deployment phase, metadata are generated by tools used by domain experts for describing data sets, variables, validation rules and more. Metadata are deployed at specific metadata servers where they are accessible to internal production systems and partly to external users. This metadata infrastructure supports identifiers at the survey level, versioning and partitioning along observation areas (for accommodating resources specific to a single statistical institute). 

\subsubsection{Tools and infrastructure}


For data validation, a number of generic design and production tools exist. Figure~\ref{estattools} shows how those tools interact through the metadata infrastructure and the exchange of XML documents:

\begin{figure}[!ht]
\begin{center}
\includegraphics[scale=0.5]{fig/estattools.png} 
\end{center}
\caption{eSTATISTIK tools and their interaction}
\label{estattools}
\end{figure}

\begin{itemize}

\item
Domain experts use \textit{Data Edit Designer} for specifying data models for validation and editing (consisting of variables and variables groups, called \textit{topics}) and validation rules which are mainly used in web forms and back-end systems. Those specifications are exported in \textit{DatML/EDT} format. Validation rules can also be exported as Java classes.

\item
The \textit{Forms Designer} is used for designing web (and paper) forms. It resues data model and rule specifications made with the Data Edit Designer. Forms pecifications are exported in \textit{DatML/ASK} format.

\item
The \textit{Survey Designer} is used for specifying survey data models and front-end validation rules for the B2B data collection system \textit{CORE}.

\item
The \textit{Date Edit Runtime} is a generic platform for perfoming data validation. It is configured through DatML/EDT documents which are deployed along with data to be validated.

\end{itemize}

The design tools are part of the \textit{.BASE} tool suite. They are run as clients of a local .BASE server that provides central storage for metadata objects. A replication database is used for distributing metadata objects among statistical institutes. .BASE servers and the replication database provide the infrastructure for distributed development.

The \textit{Survey Database} serves metadata objects in XML format to production systems. A separate external instance of this database can be accessed by external users, specifically IT service and product providers and respondents.

Figure~\ref{estatinfra} shows the overall infrastructure for the data collection design and production processes:

\begin{figure}[!ht]
\begin{center}
\includegraphics[scale=0.5]{fig/estatinfra.png} 
\end{center}
\caption{eSTATISTIK infrastructure for data collection design and production}
\label{estatinfra}
\end{figure}

\subsubsection{Specification language}

The eSTATISTIK specification language provides instructions for validating and editing data. There exist five fundamental programmatical objects falling into the categories of control, assertion and manipulation, and reuse, as shown in figure~\ref{estatspecl}:

\begin{figure}[!ht]
\begin{center}
\includegraphics[scale=0.5]{fig/estatspecl.png} 
\end{center}
\caption{eSTATISTIK data validation and editing specification language}
\label{estatspecl}
\end{figure}

Awarenesss of those contexts is very important since they correspond to named programmatic objects that are edited and managed separately in the Data Edit Designer and Data Edit Runtime. They have different and overlapping instruction subsets, but some coding and runtime interdependencies apply.

\begin{description}

\item[Procedures] serve to control the execution path of validation rules and automated edits, or in other words, to create and execute validation flows. Procedures can use the full instruction set of the specification language. Procedures can invoke other procedures conditionally as well as unconditionally, so that validation flows can be scoped and designed as building blocks, providing a means for modularisation and composition of data validation flows.

\item[Checks](a.k.a. validation rules) contain instructions for validating one or more columns/fields of the (hierarchical set of) record(s) currently in scope. Checks can only be invoked from procedures and have a very limited instruction set. They return TRUE if, and only if they encounter an error, otherwise FALSE. Only checks are evaluated for compiling validation statistics or metrics.

\item[Edits] are only invoked implicitly by checks that encounter an error (hard check). There is no instruction for invoking edits explicitly. An edit rule is therefore always bound to exactly one check and also has a very limited instruction set.

\item[Functions] provide a way for creating reusable units of code. Functions dispose of the full instructions set expect that checks cannot be invoked from functions. Functions can be invoked everywhere and recursively.

\end{description}

\paragraph{}
In order for a validation scenario to be executable, at least one rule and one procedure invoking the rule must exist.

\subsubsection{Performing data validation}

In the generic execution environment Data Edit Runtime, programmatic objects such as validation rules, reference data and other resources are bound to a survey node. Different versions of one resource can exist. A survey is associated with one or more reference periods. Reference periods serve to associate the data to be validated with (a version of) the resources used to perform the validation. For a given reference period, exactly one data set under validation can exist.

During execution of a validation flow, only the current record or the current set of hierarchical records (such as household and associated person records) of the data set under validation is visible and can be read and write accessed. Iterating over a complete data set is only possible in procedures and functions and if the data set is defined as reference data.

The Data Edit Runtime is the preferred tool for many small and medium size surveys as it provides full data validation functionality without any need for programming. However, its performance still has to be improved to make it suitable for large data volumes. Bigger surveys typically perform data validation inside the domain application but integrate validation rules defined in the Data Edit Designer through its Java export mechanism.

\subsubsection{Particularities of the PoC}

Files that serve as procedures have control in their file name, while all other .txt files represent simple validation rules.


The data used in the PoC provide some cases in which validation cannot be performed due to missing data. Those cases are recognisable by the value \code{undecided} in the column \code{expected} or in the data set name (where the full data set is validated), respectively. The idea behind this is to find out how systems differ in handling missing values. In eSTATISTIK, after executing a validation rule, the associated data are always considered validated, because validation rules only return TRUE or FALSE. Any precondition that may lead to a validation rule not being executed must be checked outside the rule, that is, in a procedure. For a detailed explanation, please see the eSTATISTIK implemenation of Rule 01
in \ref{appendix}.
